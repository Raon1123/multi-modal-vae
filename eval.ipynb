{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import imageio\n",
    "import numpy as np\n",
    "import argparse\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import cv2 as cv\n",
    "import glob as glob\n",
    "from numpy import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './VAE/MNIST/'\n",
    "\n",
    "data = torch.load(os.path.join(data_dir,'generated_data.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "data_list = [x.detach().numpy() for x in data['x']]\n",
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_dir):\n",
    "    images = []\n",
    "    for fn in os.listdir(image_dir):\n",
    "        ext = os.path.splitext(fn)[1].lower()\n",
    "        img_path = os.path.join(image_dir, fn)\n",
    "        img = imageio.imread(img_path)\n",
    "        # calculate per-channel means and standard deviations\n",
    "        means = img.mean(axis=(0, 1), dtype='float64')\n",
    "        stds = img.std(axis=(0, 1), dtype='float64')\n",
    "        # per-channel standardization of pixels\n",
    "        pixels = (img - means) / stds\n",
    "        pixels = clip(pixels, -1.0, 1.0)\n",
    "        images.append(pixels)\n",
    "    return images\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / np.expand_dims(e_x.sum(axis=1), axis=1)   # only difference\n",
    "\n",
    "def crop10x10(in_path, out_path):\n",
    "    # mnist\n",
    "    x_cors = [2,32,62,92,122,152,182,212,242,272]\n",
    "    y_cors = [2,32,62,92,122,152,182,212,242,272]\n",
    "    img_size = 28\n",
    "    number_channel = 1\n",
    "\n",
    "    print(out_path)\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    in_list = glob.glob(in_path + \"*.png\")\n",
    "    count = 0\n",
    "    for img_name in in_list:\n",
    "        count += 1\n",
    "        if (number_channel == 1):\n",
    "            img = cv.imread(img_name, 0)\n",
    "        else:\n",
    "            img = cv.imread(img_name, 1)\n",
    "        for x in x_cors:\n",
    "            for y in y_cors:\n",
    "                img_crop = img[x:x + img_size, y:y + img_size]\n",
    "                #print(img_crop.shape)\n",
    "                if (number_channel == 1):\n",
    "                    h, w = img_crop.shape\n",
    "                else:\n",
    "                    h, w, c = img_crop.shape\n",
    "                if (h != img_size) or (w != img_size):\n",
    "                    print(\"ERROR!!!\")\n",
    "                    exit()\n",
    "\n",
    "                out_name = out_path + str(count) + \"_\" + str(x) + \"_\" + str(y) + \".png\"\n",
    "                #print(out_name)\n",
    "                cv.imwrite(out_name, img_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preds2score(preds, splits=10):\n",
    "    scores = []\n",
    "    for i in range(splits):\n",
    "        part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]\n",
    "        kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
    "        kl = np.mean(np.sum(kl, 1))\n",
    "        scores.append(np.exp(kl))\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "\n",
    "def get_inception_score(images):\n",
    "    num_splits=10\n",
    "    batch_size = 100\n",
    "    model_dir = './models/checkpoints/mnist_model_10.ckpt'\n",
    "    splits = num_splits\n",
    "    inps = []\n",
    "    input_transform = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    for img in images:\n",
    "        img = img.astype(np.float32)\n",
    "        inps.append(np.expand_dims(img, 0))\n",
    "    preds = []\n",
    "    #pred_acc = []\n",
    "    n_batches = int(math.ceil(float(len(inps)) / float(batch_size)))\n",
    "    n_preds = 0\n",
    "\n",
    "    net = ResNet18().cuda()\n",
    "    net.load_state_dict(torch.load(model_dir))\n",
    "    print(\"load model successfully\")\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        sys.stdout.write(\".\")\n",
    "        sys.stdout.flush()\n",
    "        inp = inps[(i * batch_size):min((i + 1) * batch_size, len(inps))]\n",
    "        inp = np.concatenate(inp, 0)\n",
    "        inp = np.expand_dims(inp, axis=1)\n",
    "        inp = torch.from_numpy(inp).cuda()\n",
    "        outputs = net(inp)\n",
    "        pred = outputs.data.tolist()\n",
    "        pred = softmax(pred)\n",
    "        preds.append(pred)\n",
    "        n_preds += outputs.shape[0]\n",
    "    preds = np.concatenate(preds, 0)\n",
    "    preds = np.exp(preds) / np.sum(np.exp(preds), 1, keepdims=True)\n",
    "    mean_, std_ = preds2score(preds, splits)\n",
    "    return mean_, std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model successfully\n",
      "....................................................................................................\n",
      "Inception mean:  1.0063228150435726\n",
      "Inception std:  0.0037650017228050324\n"
     ]
    }
   ],
   "source": [
    "mean, std = get_inception_score(data_list)\n",
    "print('\\nInception mean: ', mean)\n",
    "print('Inception std: ', std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MnistClassifierModelBase'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m load_model\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Convolution2D, Dense, Dropout, Flatten, MaxPooling2D\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mMnistClassifierModelBase\u001b[39;00m \u001b[39mimport\u001b[39;00m MnistClassifier\n\u001b[1;32m     10\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMnistClassifier06\u001b[39;00m(MnistClassifier):\n\u001b[1;32m     11\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'MnistClassifierModelBase'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy\n",
    "from keras import Sequential\n",
    "from tensorflow.keras.models import load_model, model_from_json\n",
    "from keras.layers import Convolution2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "\n",
    "from MnistClassifierModelBase import MnistClassifier\n",
    "\n",
    "\n",
    "class MnistClassifier06(MnistClassifier):\n",
    "    def __init__(self, verbose: bool = False):\n",
    "        super().__init__(verbose)\n",
    "        self.model = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "        self.model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Dropout(0.25))\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(128, activation='relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "        self.verbose_log(self.model.summary())\n",
    "        self.compile_model()\n",
    "\n",
    "    def compile_model(self):\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    def fit(self, x, y, batch_size=32, epochs=30):\n",
    "        history = self.model.fit(x, y, batch_size, epochs)\n",
    "        return history.history['loss'], history.history['acc']\n",
    "\n",
    "    def train_on_batch(self, x: numpy.ndarray, y: numpy.ndarray, batch_size=32):\n",
    "        num_samples = x.shape[0]\n",
    "        if num_samples % batch_size != 0:\n",
    "            raise RuntimeWarning('Batch size does not divide number of samples exactly. '\n",
    "                                 'Last set of samples will not be used for training')\n",
    "        loss, accuracy = [], []\n",
    "        self.verbose_log('iteration, loss, accuracy')\n",
    "        for i in range(int(math.floor(num_samples / batch_size))):\n",
    "            data = x[i * batch_size:(i + 1) * batch_size, :, :]\n",
    "            labels = y[i * batch_size:(i + 1) * batch_size]\n",
    "            batch_loss, batch_accuracy = self.model.train_on_batch(data, labels)\n",
    "            loss.append(batch_loss)\n",
    "            accuracy.append(batch_accuracy)\n",
    "            self.verbose_log('{0:04},{1:2.4f},{2:0.4f}\\n'.format(i + 1, batch_loss, batch_accuracy))\n",
    "        return loss, accuracy\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        score = self.model.evaluate(x, y)\n",
    "        return score[0], score[1]\n",
    "\n",
    "    def classify(self, x):\n",
    "        y = self.model.predict(x)\n",
    "        return y\n",
    "\n",
    "    def save_model(self, save_path):\n",
    "        self.model.save(save_path)\n",
    "\n",
    "    def save_model_data(self, json_path, weights_path):\n",
    "        with open(json_path, 'w') as json_file:\n",
    "            json_file.write(self.model.to_json())\n",
    "        self.model.save_weights(weights_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(model_path) -> MnistClassifier:\n",
    "        model = load_model(model_path)\n",
    "        classifier = MnistClassifier06()\n",
    "        classifier.model = model\n",
    "        return classifier\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model_from_data(json_path, weights_path) -> MnistClassifier:\n",
    "        with open(json_path, 'r') as json_file:\n",
    "            model = model_from_json(json_file.read())\n",
    "        model.load_weights(weights_path)\n",
    "        classifier = MnistClassifier06()\n",
    "        classifier.model = model\n",
    "        classifier.compile_model()\n",
    "        return classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
